{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quanti_20240615.zip\n",
      "Quanti_20240606.zip\n",
      "Quanti_20240614.zip\n",
      "Quanti_20240626.zip\n",
      "Quanti_20240624.zip\n",
      "Quanti_20240627.zip\n",
      "Quanti_20240611.zip\n",
      "Quanti_20240619.zip\n",
      "Quanti_20240609.zip\n",
      "Quanti_20240605.zip\n",
      "Quanti_20240623.zip\n",
      "Quanti_20240607.zip\n",
      "Quanti_20240620.zip\n",
      "Quanti_20240630.zip\n",
      "Quanti_20240617.zip\n",
      "Quanti_20240603.zip\n",
      "Quanti_20240602.zip\n",
      "Quanti_20240621.zip\n",
      "Quanti_20240629.zip\n",
      "Quanti_20240625.zip\n",
      "Quanti_20240601.zip\n",
      "Quanti_20240612.zip\n",
      "Quanti_20240618.zip\n",
      "Quanti_20240616.zip\n",
      "Quanti_20240610.zip\n",
      "Quanti_20240604.zip\n",
      "Quanti_20240613.zip\n",
      "Quanti_20240628.zip\n",
      "Quanti_20240622.zip\n",
      "Quanti_20240608.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Caminho da pasta que deseja listar os arquivos\n",
    "caminho_pasta = \"Arquivos Diários\"\n",
    "\n",
    "# Listar todos os arquivos na pasta\n",
    "arquivos = os.listdir(caminho_pasta)\n",
    "\n",
    "# Mostrar os arquivos\n",
    "for arquivo in arquivos:\n",
    "    print(arquivo)\n",
    "\n",
    "\n",
    "    # Caminho do arquivo zip\n",
    "    zip_file_path = 'Arquivos Diários/{}'.format(arquivo)\n",
    "\n",
    "    # Abrir e extrair o conteúdo do arquivo zip\n",
    "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "        # Extrair todos os arquivos\n",
    "        zip_ref.extractall('arquivos_diarios_extraidos')\n",
    "\n",
    "        # Ler o arquivo CSV extraído\n",
    "        # csv_file_path = 'arquivos_diarios_extraidos/{}.csv'.format(arquivo)\n",
    "        # df = pd.read_csv(csv_file_path)\n",
    "\n",
    "# Exibir as primeiras linhas do DataFrame\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "zip_file_path = 'Lote Inicial/{}'.format(\"LoteInicial.zip\")\n",
    "\n",
    "# Abrir e extrair o conteúdo do arquivo zip\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    # Extrair todos os arquivos\n",
    "    zip_ref.extractall('Lote Inicial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[141], line 36\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, elem \u001b[38;5;129;01min\u001b[39;00m df1\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     35\u001b[0m     df0 \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m---> 36\u001b[0m     arquivos_dados \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdados\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(elem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01min\u001b[39;00m arquivos_dados:\n\u001b[1;32m     38\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdados/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m elem[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mID\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m arquivo_json:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import json  \n",
    "import os   \n",
    "\n",
    "def verifica_dados(elem_atual, json_antigo, id):\n",
    "    # A função verifica_dados compara os dados atuais com os dados antigos.\n",
    "    elem_antigo = json_antigo[id]  # Obtém o elemento antigo correspondente ao ID\n",
    "\n",
    "    # Itera sobre cada chave e valor do elemento antigo\n",
    "    for chave, valor in elem_antigo.items():\n",
    "        # Compara o valor atual com o valor antigo (convertendo ambos para string)\n",
    "        if str(elem_atual[chave]) != str(valor):\n",
    "            # Extrai ISIN e data do ID (que é composto por ISIN e data)\n",
    "            isin = id.split('_')[0]\n",
    "            data = id.split('_')[1]\n",
    "            # Cria um dicionário com os dados a serem registrados\n",
    "            dados = {\n",
    "                'ISIN': [isin],\n",
    "                'campo_alterado': [chave],\n",
    "                'valor_antigo': [valor],\n",
    "                'valor_novo': [elem_atual[chave]],\n",
    "                'data': [data],\n",
    "            }\n",
    "            # Verifica se o arquivo CSV já existe\n",
    "            if os.path.exists('results/serie_valores_substitutos.csv'):\n",
    "                # Se existir, anexa os novos dados ao arquivo CSV\n",
    "                pd.DataFrame(dados).to_csv('results/serie_valores_substitutos.csv', mode='a', header=False, index=False)\n",
    "            else:\n",
    "                # Se não existir, cria um novo arquivo CSV\n",
    "                pd.DataFrame(dados).to_csv('results/serie_valores_substitutos.csv', index=False)\n",
    "            # Atualiza o valor antigo no dicionário com o novo valor\n",
    "            elem_antigo[chave] = valor\n",
    "    \n",
    "    json_antigo[id] = elem_antigo  # Atualiza o dicionário antigo com os valores modificados\n",
    "\n",
    "    # Salva os dados atualizados em um arquivo JSON\n",
    "    with open('dados/{}.json'.format(id), 'w') as arquivo_json:\n",
    "        json.dump(df0, arquivo_json, indent=4, ensure_ascii=False)  # Escreve o dicionário atualizado no arquivo JSON\n",
    "\n",
    "# Dicionário pode ser consideravelmente grande, no caso de executar com isso em grandes dados\n",
    "# Fazer um sistema de guardar em arquivos .json, onde é possível verificar as entradas um a um.\n",
    "for elem in range(len(arquivos)):\n",
    "    # Lê os arquivos CSV diários extraídos e armazena em um DataFrame\n",
    "    df1 = pd.read_csv('arquivos_diarios_extraidos/{}'.format(arquivos[elem]))\n",
    "    # Cria uma nova coluna 'ID' que combina 'ISIN' e 'AuM Fund Date'\n",
    "    df1['ID'] = df1['ISIN'] + '_' + df1['AuM Fund Date']\n",
    "    # Remove linhas onde 'ISIN' ou 'ID' são nulos\n",
    "    df1 = df1.dropna(subset=['ISIN', 'ID'])\n",
    "    # Remove duplicatas na coluna 'ID', mantendo a última ocorrência\n",
    "    df1.drop_duplicates('ID', keep='last')  # perde info porém é para dados entre dias\n",
    "\n",
    "    # Itera sobre cada linha do DataFrame df1\n",
    "    for _, elem in df1.iterrows():\n",
    "        df0 = {}  # Inicializa um dicionário vazio para armazenar os dados antigos\n",
    "        arquivos_dados = os.listdir('dados')  # Lista todos os arquivos no diretório 'dados'\n",
    "\n",
    "        # Verifica se um arquivo JSON correspondente ao ID já existe\n",
    "        if str(elem['ID'] + '.json') in arquivos_dados:\n",
    "            # Se existir, carrega os dados antigos do arquivo JSON\n",
    "            with open('dados/' + elem['ID'] + '.json', 'r') as arquivo_json:\n",
    "                df0 = json.load(arquivo_json)\n",
    "            # Chama a função para verificar os dados\n",
    "            verifica_dados(elem, df0, elem['ID'])\n",
    "        else:\n",
    "            # Se não existir, cria uma nova entrada no dicionário df0\n",
    "            df0[elem['ID']] = {\n",
    "                'FE Bid': elem['FE Bid'],\n",
    "                'AuM Fund': elem['AuM Fund'],\n",
    "                'AuM Share Class': elem['AuM Share Class'],\n",
    "                'Bid NAV': elem['Bid NAV'],\n",
    "                'Ask NAV': elem['Ask NAV'],\n",
    "                'Valuation NAV': elem['Valuation NAV'],\n",
    "                'NoS Share Class': elem['NoS Share Class'],\n",
    "                'NoS Fund': elem['NoS Fund'],\n",
    "                'FE Offer': elem['FE Offer'],\n",
    "                'FE NAV': elem['FE NAV'],\n",
    "                'FE AUM Fund': elem['FE AUM Fund'],\n",
    "                'FE Generic NAV': elem['FE Generic NAV'],\n",
    "                'Transaction NAV': elem['Transaction NAV'],\n",
    "            }\n",
    "            # Cria um novo arquivo JSON para os dados se não existir\n",
    "            with open('dados/{}.json'.format(elem['ID']), 'w') as arquivo_json:\n",
    "                json.dump(df0, arquivo_json, indent=4, ensure_ascii=False)  # Escreve o dicionário no arquivo JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quantum_hackathon",
   "language": "python",
   "name": "quantum_hackathon"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
